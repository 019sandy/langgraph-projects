{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c9c7da",
   "metadata": {},
   "source": [
    "## ``Langgraph``\n",
    "1. Graph :\n",
    "2. State Machine : manage sequence \n",
    "    - State are Nodes\n",
    "    - Transition are Edges\n",
    "3. Langgraph : \n",
    "    - can describe our flows using nodes and Edges , and can build Agentic Application\n",
    "    \n",
    "4. Flow Engineering :\n",
    "    - complex decison making nodes and edges , may be iterative cycle\n",
    "    - guides AI system with guided steps , that mimics human software Engineering\n",
    "    - We humans will define the FLow. LLM will take decisions at interim level and LLM will \n",
    "      work within the FLow that we created.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098c760",
   "metadata": {},
   "source": [
    "## ``Langgraph Core Components``\n",
    "1. ``Node``: python functions - Any Python Code\n",
    "2. ``Edges``: Edges connects those nodes within the graph execution\n",
    "3. ``Conditional Edges``: They help making decisons weather to go to Node A , or to Node B\n",
    "4. ``built in Nodes`` : start ( entrypoint of graph execution ) and end ( last node )\n",
    "\n",
    "5. ``Agent State``: It is a ``Dictionary`` that keeps information that we need to keep track in the graph . Like some node results , chat history , etc. etc. \n",
    "    - It is local to the graph and is available only during the Graph Execution.\n",
    "    - Available on each edge as well\n",
    "    - We can keep the ``State`` in ``Persistence`` , so it is not temporarily available\n",
    "\n",
    "### ``Nodes`` will always receives as the Input the current Graph State ( state: GraphState) which is all the information the Node needs in order to do it's work. and it always returns the keys of the Dictionary of we ultimately wants to update in the State once node is Executed.\n",
    "\n",
    "\n",
    "6. ``Cyclic Graph`` : Loops\n",
    "7. ``Human in the Loop`` : human apprvals \n",
    "8. ``Persistence`` : Langgraph has built in functions which helps us persist the State of the Graphs.\n",
    "\n",
    "\n",
    "- ``Course here`` - https://github.com/emarco177/langgraph-course/tree/main\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d45522",
   "metadata": {},
   "source": [
    "## ``Section 2 - Reflection Agent``\n",
    "1. Reflection Agents enhance the quality of success rates of Agents answers . They ask LLM to reflect to its responses and allows it to learn and improve it over time.\n",
    "2. What we will do here :\n",
    "    - Reflection Agent will take our Tweet\n",
    "    - It will give us a critique with a reflection step\n",
    "    - Will give the feedback and criticise our twitter post\n",
    "    - We will take this feedback , feed it into LLM adn will ask it to revise.\n",
    "    - Revision number 1 and this will continue.\n",
    "\n",
    "## ``Section 2.1 - Project Setup``\n",
    "1. Create project directory :\n",
    "    - mkdir reflection-agent\n",
    "    - cd to this directory\n",
    "2. use Poetry to create Virtual Env :\n",
    "    - poetry init , and just hit enter for all options\n",
    "    - It generates project.toml file with basic structure\n",
    "    - Install all dependencies in the poetry env now:\n",
    "        - poetry add python-dotenv black isort langchain langchain-openai langgraph\n",
    "        - it generates project.toml file as base file , and whatever we will install will get added here , poetry.   lock will have all installation details.\n",
    "3. Install dependencies\n",
    "4. configure our VSC\n",
    "5. env to load Langsmith API keys , OpenAI API KEys , etc \n",
    "    - create .env file with OpenAI key , LangSmith Key , LAngchain key , LAngchain Project\n",
    "    - open ai APi key - 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\n",
    "    - Langsmith APU KEy - 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\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553b79f",
   "metadata": {},
   "source": [
    "## ``12. Creating The Reflector Chain and Tweet Revisor Chain``\n",
    "- Chains - ``generation chain`` and ``reflect_chain``\n",
    "- generate chain  wil l generate the tweet\n",
    "- reflect chain will critique it and suggest us to make it better and better\n",
    "- and every Iteration we will feed this critique into generation chain\n",
    "\n",
    "- ``chains.py`` :\n",
    "1. ``ChatPromptTemplate`` : it holds our information that we send as ``humans`` to LLM or the answer taht we will receive from ``AI`` . Object of ChatRPomptTemplate class will hold it.\n",
    "2. ``MessagesPlaceholder``: This class will hold the future messages that we are going to get\n",
    "3. ``reflect_prompt`` : This will act as a critique .It will review the output ( in this case Twitter post )and it will criticze it and will say how it can be better\n",
    " - here we will also add ``Messageplaceholder`` - to keep history messages that our agent is going to invoke to criticze and get recommendations again and again\n",
    "4. ``generation_prompt``: it will read from the critique mesaGE AND REGENERATE THE TEWWT . rewrites the tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278859c3",
   "metadata": {},
   "source": [
    "## ``13. Defining Our LangGraph Graph``\n",
    "- define Nodes , define Edges , define Conditional edges , compile graph , visualize graph\n",
    "1. main.py file will have Graph Implementation\n",
    "2. Do some Imports (from typing(list and sequnece), from langgraph (humanmessages, BaseMessage), langgraphgraph start , end , MessageGraph) , chains that we have buikt\n",
    "3. ``MessageGraph``: IT is a type of graph whose state is sequence of messages. In this graph every node will receive list of messages (StateGraph) which is the input for every node\n",
    "    - It receives a list of messages as input.\n",
    "    - Returns one or more messages as output.\n",
    "4. ``NODES`` :  Default Start and End node + reflect node + generate node.\n",
    "5. ``generation node function`` : Will receive as an input the state. In langgraph's MessageGraph -> state is simply a sequence of messages. It will run our Generation chain , will invoke/return it with all the state. and as we have seen in chains , state is messages ( or history as what has happened in the agent ) as we kept in MessagePlacholder.\n",
    "- Kanggraph will then take teh return values and will append it to the list of messages \n",
    "\n",
    "6. ``reflection node function``: it will also receives as input state . again as in Langgrapbh MEssages graph state is a sequence of BaseMessage.Beware reflection chain will be dependent on previous messages , so here isntead of passsing the state we will pass the messages as we must have got from previous generation chain.\n",
    "- when LLM is invoked we take the output and transform it into HumanMessage from AI Message \n",
    "\n",
    "7. Initialize the graph - MessageGraph\n",
    "8. Add nodes to it ( Key and value) means variablename & python function name \n",
    "9. tell Langgraph which will be our Starting Node\n",
    "\n",
    "10. we should decide after generate function should it end or continue to reflect . That ``should continue`` function again will receive all the messages as state and will decide . which node it should go to (end node or Reflection node) . Return value of this is String and is the key of the node we want to go to. In ``Langgraph`` it is called as ``Conditional Edge`` . And this should continue function will decide which node will it go to next.\n",
    "\n",
    "11. def ``should_continue`` node. in this example we are not letting LLM decide weather to finish or decide to next step else we are giving it a go if there are how many nodes it has encountered so far  which is stored in STATE , so if it is > 6 that means ( generate -> reflect -> generate -. reflect -> generate -. reflect ). it means enough of refinement we can end and if any less again reflect\n",
    "\n",
    "12. now we will tell Langgraph that once tweet is generated we will add a conditional edge , in which shoukd_continue function will determin which node shoudl it go to next \n",
    "13. we will also create an edge between REFLECT and GENERATE NODE\n",
    "\n",
    "14. comiple the graph = graph object.compile()\n",
    "\n",
    "15. visualise the graph print(graph.get_graph().draw_mermaid())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
